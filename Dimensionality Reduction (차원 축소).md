### Dimensionality Reduction (차원 축소)

차원이란 :  데이터를 나타내는 feature의 수

예시 : 하나의 이미지를 하나의 샘플로 보고  그 이미지를 구성하는 pixel의 정보를 데이터로 받았다면, 아래의 두 그림 중에 상단에 위치한 그림은 520 x 339 (= 176,280)의 feature로 구성되어있는 이미지로 볼 수 있다. 반면에 하단의 그림은 33 x 22 (= 726)의 feature로 구성되어 있다는 것을 알 수가 있다. 두 이미지를 비교해본다면 당연히 보다 큰 차원으로 구성되어있는, 즉 보다 많은 정보를 가지고 있는 이미지가 더욱 선명하게 보인다.

![냥이](C:\Users\a\Desktop\BOAZ\머신러닝 스터디\냥이.jpg)

![냥이2](C:\Users\a\Desktop\BOAZ\머신러닝 스터디\냥이2.jpg)



- 차원 축소의 필요성

  차원이 크다는 것은 한 샘플에 보다 많은 정보가 있다는 뜻이므로 분석에 있어서 도움이 된다. 그렇다면 차원의 축소, 즉 고차원의 데이터를 저차원의 데이터로 만들어야 할 필요가 있는지 의문이 생길 수가 있다.  그 필요성의 가장 큰 이유는 '차원의 저주(Curse of dimensionality)'이다. 차원의 저주는 샘플의 수는 동일하다는 가정하에서 데이터의 차원이 증가할 수록 데이터가 가지는 부피는 기하급수적으로 증가하는 반면에 샘플의 수가 일정하므로 데이터의 밀도가 급격히 낮아진다.  이러한 문제가 발생할 경우에 feature들이 나타내는 공간을 충분히 반영할 만큼의 샘플이 없다면, 일부의 샘플이 가진 특징적인 노이즈까지 학습되어 과적합이 발생하는 문제점이 생기게 된다. 따라서 이런 문제점을 해결하기 위해서 차원을 축소하게 된다. (추가적으로 차원을 축소하게 되면 우리가 인지할 수 있는 1-4차원의 시각화가 가능하다는 장점이 있다.)

  

- Feature Selection (변수 선택)

  데이터 상에 존재하는 feature들 중에 분석에 도움이 될만한 일부의 feature만을 골라내는 방법이다.  따라서 가지고 있는 사전지식으로 보았을 때에 우리가 목적하는 바와 상관없는 변수들을 제거하는 방법도 있고, 각 변수의 유의미 정도와 변수 간의 상관관계가 얼만큼 큰지 등을 확인하여서 분석에 도움이 안되는 변수들을 제거할 수도 있다. 또한 설정한 모델이 자동적으로 변수를 선택하는 과정을 거칠 수도 있다. (예를 들어 LASSO)

  

- Feature Extraction (변수 추출)

  feature selection이 전체 변수 중에서 일부의 변수만을 부분적으로 선택하는 방법이라면, feature extraction은 변수들이 나타내는 고차원 공간 상에서 데이터의 특징을 잘 표현할 수 있는 저차원 공간을 찾아내어 고차원 공간 상의 데이터를 우리가 찾은 저차원 공간 상에 투영시키는 방법이다. 이 저차원 공간은 단순하게 원래의 변수가 나타내는 축으로 나타난 공간이 아닌 원래의 변수들 간에 선형적 / 비선형적 결합을 통해 새로 생성되는 변수를 축으로 하는 공간이다.

  

- PCA (Principal Component Analysis, 주성분 분석)

  PCA는 feature extraction하는 방법 중에 원래 feature들을 선형 결합을 통해 새로운 feature를 만드는 방법에 속하며, unsupervised learning에 속한다. 기본적인 개념만을 설명하자면 데이터가 나타내는 분산을 최대한으로 유지하는 선형적인 축을 찾아 보다 저차원 공간에 사영시키게 된다. 원 데이터상에서의 labeling과는 상관없이 분산이 기준이 되기 때문에 unsupervised learning에 속하게 된다. 아래의 그림은 각 파란 점으로 나타난 데이터들이 회전하는 축(선형결합을 통해 새로 생성된 축)에 대해 빨간 점으로 사영되고 있다.  회전하는 축이 분홍색 선분과 맞닿게 되었을 때에 빨간 점들이 가장 넓게 분포되어 있음을 알 수가 있다. 이는 원 데이터 상의 분산이 최대로 유지되는 공간이라는 말과 같다. 따라서 PCA는 이런 공간을 찾는 분석 방법이라고 할 수가 있다. 

  (수학적으로 접근하기 위해서는 선형대수학에 대한 어느정도의 이해가 필요하므로 생략한다. 혹시 필요하다면 https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/ 에서 참고하길 바람. 아래의 그림 출처 또한 위의 주소이다. )

![img](file:///D:/study/3%ED%95%99%EB%85%84%202%ED%95%99%EA%B8%B0/%EB%8B%A4%EB%B3%80%EB%9F%89/lecturenote/ch04.%20PCA/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D(Principal%20Component%20Analysis)%20%C2%B7%20ratsgo's%20blog_files/Uv2dlsH.gif)

- LDA (Linear Discriminant Analysis, 선형판별분석)

  PCA와는 다르게 LDA는 labeling 정보를 사용하게 된다. 따라서 supervised learning이며, 각 class를 구분하는 선형적 결정경계를 찾는 것이 LDA 방법이다.  수식적인 설명을 제외하고, 개념적인 설명만 하자면 class간의 평균거리는 멀도록하면서 class 내에서의 분산을 작게 만드는 것을 목적으로 한다. 그렇게 되는 결정경계를 찾은 후에 결정경계에 수직인 축을 공간으로 가지는 저차원 공간으로 사영시킨다면 차원축소가 가능하게 되는 것이다.

  ![img](http://www.kwangsiklee.com/wp-content/uploads/2017/12/lda0400.png)



머신러닝을 위한 기본적인 선형대수학 정리 : http://www.kwangsiklee.com/2017/10/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%97%90-%ED%95%84%EC%9A%94%ED%95%9C-%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%ED%95%B5%EC%8B%AC-%EC%A0%95%EB%A6%AC/